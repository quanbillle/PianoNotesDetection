{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORT LIBRARIES\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import keras\n",
    "from tensorflow.keras import activations\n",
    "import copy\n",
    "from tensorflow.python.client import device_lib\n",
    "#tf.config.list_physical_devices(\"GPU\")\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORT SUBDATASET OF TRAINING, TESTING DATASET\n",
    "\"\"\"\n",
    " \n",
    "train = json.load(open(os.path.join(os.getcwd(), \"dataset/train/notev7.json\")))\n",
    "val = json.load(open(os.path.join(os.getcwd(), \"dataset/val/notev7.json\")))\n",
    "test = json.load(open(os.path.join(os.getcwd(), \"dataset/test/notev7.json\")))\n",
    "\n",
    "trainStaff = json.load(open(os.path.join(os.getcwd(), \"dataset/train/StaffAndSep.json\")))\n",
    "valStaff = json.load(open(os.path.join(os.getcwd(), \"dataset/val/StaffAndSep.json\")))\n",
    "testStaff = json.load(open(os.path.join(os.getcwd(), \"dataset/test/StaffAndSep.json\")))\n",
    "\n",
    "\n",
    "\n",
    "def extractData(datasetStaff, datasetNotes, dirn = None):\n",
    "    mask = []\n",
    "    TotalSortedList = []\n",
    "    for data in datasetStaff:\n",
    "        annotation = [a for a in datasetStaff[data]['regions']]\n",
    "        sortedList = [i for i in range(len(annotation))]\n",
    "        for k_index, a in enumerate(sortedList): \n",
    "            index  = k_index\n",
    "            if index != len(sortedList) -1:\n",
    "                while index < len(sortedList)-1:\n",
    "                    if annotation[a]['shape_attributes']['y'] > annotation[sortedList[index+1]][\"shape_attributes\"][\"y\"]:\n",
    "                        i = sortedList[index+1]\n",
    "                        sortedList[index+1] = sortedList[index]\n",
    "                        sortedList[index] = i\n",
    "                        break\n",
    "                    index += 1\n",
    "\n",
    "            if index > 0:\n",
    "                while index > 0:\n",
    "                    if annotation[sortedList[index]]['shape_attributes']['y'] < annotation[sortedList[index-1]][\"shape_attributes\"][\"y\"]:\n",
    "                        i = sortedList[index-1]\n",
    "                        sortedList[index-1] = sortedList[index]\n",
    "                        sortedList[index] = i\n",
    "                    index -= 1\n",
    "\n",
    "        TotalSortedList.append(sortedList)\n",
    "\n",
    "\n",
    "    TrainDataset = []\n",
    "    ValDataset = []\n",
    "    c = ['G4','A4','B4','C3','D3','E3','F3','G3','A3','B3','C2','D2','E2','F2','G2','A2','B2','C','D','E','F','G']\n",
    "\n",
    "\n",
    "    for image in datasetNotes:\n",
    "        if len(datasetNotes[image]['regions']) > 0:\n",
    "            _id = ''\n",
    "            index = 0\n",
    "            for i, a in enumerate(datasetStaff):\n",
    "                if datasetNotes[image]['filename'] == datasetStaff[a]['filename']:\n",
    "                    _id  = a\n",
    "                    index = i\n",
    "                    break\n",
    "\n",
    "            img = cv2.imread(os.path.join(os.getcwd(), f\"dataset/Notev1/{dirn}/{datasetNotes[image]['filename']}\"))\n",
    "            start, end = 0, img.shape[0]\n",
    "\n",
    "            b = 0\n",
    "            while b < len(TotalSortedList[index]):\n",
    "                Height = datasetStaff[_id]['regions'][TotalSortedList[index][b]]['shape_attributes']['height']\n",
    "                if b > 0: \n",
    "                    start = datasetStaff[_id]['regions'][TotalSortedList[index][b-1]]['shape_attributes']['y']\n",
    "\n",
    "                if b + 1 < len(TotalSortedList[index]):\n",
    "                    end = datasetStaff[_id]['regions'][TotalSortedList[index][b+1]]['shape_attributes']['y']\n",
    "\n",
    "\n",
    "                \n",
    "                for note in datasetNotes[image]['regions']:\n",
    "                    load = False\n",
    "                    if note[\"region_attributes\"]['class_ids'] in c:\n",
    "                        load = True\n",
    "\n",
    "\n",
    "                    if load == True and note['shape_attributes']['y'] > start and note['shape_attributes']['y'] + note['shape_attributes']['height'] < end:\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        x = note['shape_attributes']['x']\n",
    "                        y = note['shape_attributes']['y']\n",
    "                        w = note['shape_attributes']['width']\n",
    "                        h = note['shape_attributes']['height']\n",
    "                        sh = datasetStaff[_id]['regions'][TotalSortedList[index][b]]['shape_attributes']['height']\n",
    "                        sy = datasetStaff[_id]['regions'][TotalSortedList[index][b]]['shape_attributes']['y']\n",
    "                        \n",
    "                        def distribute(data):\n",
    "                            distri = []\n",
    "\n",
    "                            av = np.average(data)\n",
    "                            for i, r in enumerate(data):\n",
    "                                distri.append(0)\n",
    "                                for c in r:\n",
    "                                    if  0.299*c[0] + 0.587*c[1] + 0.114*c[2] < av:\n",
    "\n",
    "                                        distri[i] += 1\n",
    "\n",
    "                            distri = np.array(distri)/data.shape[1]\n",
    "                            \n",
    "                            for i,v in enumerate(distri):\n",
    "                                if v > 0.9:\n",
    "                                    distri[i] = 0\n",
    "                                else:\n",
    "                                    distri[i] = 1\n",
    "                                    \n",
    "                            return distri\n",
    "                        \n",
    "                        \n",
    "                       \n",
    "                        endm = int(sy+2*Height)\n",
    "                        startm = int(sy - Height)\n",
    "                        if endm > (int(end)):\n",
    "                            endm = int(end)\n",
    "                        \n",
    "                        if startm < start:\n",
    "                            startm = start\n",
    "                            \n",
    "                        topmost = sy-startm\n",
    "                        bottommost = y+h - startm\n",
    "                        \n",
    "                        if y-startm < topmost:\n",
    "                            topmost = y - startm\n",
    "                        \n",
    "                        if bottommost < sy + sh -startm:\n",
    "                            bottommost = sy + sh - startm\n",
    "                        \n",
    "                        \n",
    "                        #mask = np.zeros((3*sh,w,6))\n",
    "                            \n",
    "                        mask = []\n",
    "                        cmask =  copy.deepcopy(img[startm:endm,x:x+w,:])\n",
    "                        staffFilter = distribute(img[sy:sy+sh,x:x+w,:])\n",
    "            \n",
    "                        \n",
    "                        bmask = np.zeros((cmask.shape[0],cmask.shape[1],3)) + 255\n",
    "                        bmask[sy-startm: sy+sh-startm,:,:] = 0\n",
    "                        #mask.append(img[startm:endm,x:x+w,:])\n",
    "                        \n",
    "                        background = np.average(img[startm:endm,x:x+w,:]) + 255\n",
    "                        background /= 2\n",
    "                        \n",
    "                        cmask[0:sy-startm,:,:] = background\n",
    "                        cmask[sy+sh-startm: cmask.shape[0], :,:] = background\n",
    "                        \n",
    "                        bmask = copy.deepcopy(cmask)\n",
    "                        \n",
    "                        gooddata = True\n",
    "                        \n",
    "                        notem = np.zeros((h, w,3))\n",
    "                        hrw = h/w\n",
    "                        for nr in range(h):\n",
    "                            for nc in range(w):\n",
    "                                if (nr - h/2)**2 + (hrw)**2*(nc - w/2)**2 < h**2/4:\n",
    "                                    notem[nr, nc,:] = 1\n",
    "                                    \n",
    "                        try:\n",
    "                        \n",
    "                            for mr, rv in enumerate(notem):\n",
    "                                for mc,cv in enumerate(rv):\n",
    "                                    if cv[0]== 0:\n",
    "                                        cmask[y-startm+mr, mc,:] = 255\n",
    "                                    else:\n",
    "                                        cmask[y-startm+mr, mc,:] = copy.deepcopy(img[y+mr,x+mc,:])\n",
    "                        except:\n",
    "                            gooddata = False\n",
    "                            pass\n",
    "                        #print(h)\n",
    "                        \n",
    "                        #cmask[y - startm: y+h - startm,:,:] = copy.deepcopy(notem)\n",
    "                        #print(h)\n",
    "                                    \n",
    "                                    \n",
    "                        \n",
    "                                \n",
    "                                    \n",
    "                            \n",
    "                        #cmask = cv2.resize(cmask, (20,100))\n",
    "                        #bmask = cv2.resize(bmask, (20,100))\n",
    "                        \n",
    "                        def finetuneImage(data, fil):\n",
    "                            data = np.array(data)\n",
    "                            fil = np.array(fil)\n",
    "                            for i, v in enumerate(data):\n",
    "                                for j,a in enumerate(v):\n",
    "                                    data[i,j,0] *= fil[i]\n",
    "                                    data[i,j,1] *= fil[i]\n",
    "                                    data[i,j,2] *= fil[i]\n",
    "                            \n",
    "                            return data\n",
    "                        \n",
    "                        cmask[sy-startm: sy+sh-startm,:,:] = finetuneImage(cmask[sy-startm: sy+sh-startm,:,:], staffFilter)\n",
    "                        \n",
    "                        def distribute(data):\n",
    "                            distri = []\n",
    "                            av = np.average(data)\n",
    "                            for i, r in enumerate(data):\n",
    "                                distri.append(0)\n",
    "                                for c in r:\n",
    "                                    if  0.299*c[0] + 0.587*c[1] + 0.114*c[2] < av:\n",
    "\n",
    "                                        distri[i] += 1\n",
    "                                        \n",
    "                            distri = np.array(distri)/data.shape[1]\n",
    "                            \n",
    "                            #pos = [1,0]\n",
    "\n",
    "                            #for i,v in enumerate(distri):\n",
    "                            #    if v > 0.8:\n",
    "                            #        pos[0] = i\n",
    "                            #        break\n",
    "                            \n",
    "                            #for i in range(len(distri)):\n",
    "                            #    if distri[len(distri) - i-1] == 1:\n",
    "                            #        pos[1]= len(distri) - i-1\n",
    "                            #        break\n",
    "\n",
    "                            return distri\n",
    "                        \n",
    "                        spos = distribute(cmask)\n",
    "                        subsh = sh/4\n",
    "                        rjk = spos\n",
    "                        \n",
    "                        \n",
    "                        mask.append(cmask)\n",
    "                        mask.append(copy.deepcopy(img[startm:endm,x:x+w,:]))\n",
    "                      \n",
    "                        \n",
    "            \n",
    "                        \n",
    "                        \n",
    "                        if gooddata == True:\n",
    "                            TrainDataset.append({'mask': [cv2.resize(cmask, (20,100)),rjk], \"class_ids\": note[\"region_attributes\"]['class_ids']})\n",
    "                            \n",
    "                            if dirn == 'train':\n",
    "                                down = 5\n",
    "                                while bottommost + down < mask[0].shape[0]:\n",
    "                                    temcm = np.zeros(mask[0].shape) + 255\n",
    "                                    temcm[topmost+down: bottommost+down,:,:] = copy.deepcopy(mask[0][topmost:bottommost,:,:])\n",
    "                                    tembm = np.zeros(mask[1].shape) + 255\n",
    "                                    tembm[topmost+down: bottommost+down,:,:] = copy.deepcopy(mask[1][topmost:bottommost,:,:])\n",
    "                                    down += 5\n",
    "                                    TrainDataset.append({'mask': [cv2.resize(temcm, (20,100)), rjk], \"class_ids\": note[\"region_attributes\"]['class_ids']})\n",
    "                                    expandDim = np.zeros((6,temcm.shape[1],3)) + 255\n",
    "                                    #for k in range(10):\n",
    "                                    #    temcm = np.concatenate((temcm,expandDim))\n",
    "                                    #    TrainDataset.append({'mask': [cv2.resize(temcm, (20,100)), rjk], \"class_ids\": note[\"region_attributes\"]['class_ids']})\n",
    "                                    \n",
    "                                down = 5\n",
    "                                while topmost-down > 0:\n",
    "                                    temcm = np.zeros(mask[0].shape) + 255\n",
    "                                    temcm[topmost-down: bottommost-down,:,:] = copy.deepcopy(mask[0][topmost:bottommost,:,:])\n",
    "                                    tembm = np.zeros(mask[1].shape) + 255\n",
    "                                    tembm[topmost-down: bottommost-down,:,:] = copy.deepcopy(mask[1][topmost:bottommost,:,:])\n",
    "                                    down += 5\n",
    "                                    TrainDataset.append({'mask': [cv2.resize(temcm, (20,100)),rjk], \"class_ids\": note[\"region_attributes\"]['class_ids']})\n",
    "                                    expandDim = np.zeros((6,temcm.shape[1],3)) + 255\n",
    "                                    #for k in range(10):\n",
    "                                    #    temcm = np.concatenate((expandDim, temcm))\n",
    "                                    #    TrainDataset.append({'mask': [cv2.resize(temcm, (20,100)), rjk], \"class_ids\": note[\"region_attributes\"]['class_ids']})\n",
    "                                \n",
    "                            \n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "                b += 2    \n",
    "    dict_c = {}\n",
    "    for i, _class in enumerate(c):\n",
    "        dict_c[_class] = i\n",
    "\n",
    "\n",
    "    TotalMask = [[] for i in range(len(c))]\n",
    " \n",
    "    \n",
    "\n",
    "    \n",
    "    for i in TrainDataset:\n",
    "        TotalMask[dict_c[i['class_ids']]].append(i['mask'])\n",
    "    \n",
    "    return TotalMask\n",
    "\n",
    "train_dataset = extractData(trainStaff,train, 'train')\n",
    "val_dataset = extractData(valStaff, val, 'val')\n",
    "test_dataset = extractData(testStaff, test, 'test')\n",
    "\n",
    "testM_x = []\n",
    "\n",
    "test_y = []\n",
    "\n",
    "trainM_x = []\n",
    "\n",
    "train_y = []\n",
    "\n",
    "valM_x = []\n",
    "\n",
    "val_y = []\n",
    "\n",
    "for i,data in enumerate(train_dataset):\n",
    "    for k, info in enumerate(data):\n",
    "        trainM_x.append(info)\n",
    "        \n",
    "        train_y.append(i)\n",
    "        \n",
    "for i,data in enumerate(val_dataset):\n",
    "    for k, info in enumerate(data):\n",
    "        valM_x.append(info)\n",
    "        \n",
    "        val_y.append(i)\n",
    "        \n",
    "for i,data in enumerate(test_dataset):\n",
    "    for k, info in enumerate(data):\n",
    "        testM_x.append(info)\n",
    "       \n",
    "        test_y.append(i)\n",
    "        \n",
    "\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "trainM_x = np.array(trainM_x)\n",
    "\n",
    "\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "valM_x = np.array(valM_x)\n",
    "\n",
    "\n",
    "test_y = np.array(test_y)\n",
    "  \n",
    "testM_x = np.array(testM_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MERGE SUBDATASET\n",
    "\"\"\"\n",
    "def extract_infor(dataset):\n",
    "    image1 = []\n",
    "    for i, v in enumerate(dataset):\n",
    "        image1.append(v[0])\n",
    "        \n",
    "    \n",
    "    return np.array(image1)/255\n",
    "\n",
    "trainM1_x = extract_infor(trainM_x)\n",
    "testM1_x = extract_infor(testM_x)\n",
    "valM1_x = extract_infor(valM_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INSPECT DATA\n",
    "\"\"\"\n",
    "j = 2000\n",
    "#change j to view different images \n",
    "\n",
    "plt.imshow(trainM1_x[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BUILD THE PN_CNN MODEL\n",
    "\n",
    "noteFocus(): apply position factor \n",
    "conv_block(): input goes through a convolutional layer before adding to final stage \n",
    "identity_block(): input is added directly to the final stage \n",
    "\n",
    "model's layers: input -> noteFocus -> CNN -> FCNN\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import layers\n",
    "c = ['G4','A4','B4','C3','D3','E3','F3','G3','A3','B3','C2','D2','E2','F2','G2','A2','B2','C','D','E','F','G']\n",
    "\n",
    "class noteFocus(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(noteFocus, self).__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.space = np.zeros((100, 20,3))\n",
    "       \n",
    "        for i in range(self.space.shape[0]):\n",
    "            self.space[i,:,:] = i + 50\n",
    "        self.space = self.space/150\n",
    "        self.space = tf.convert_to_tensor(self.space, dtype = tf.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_w(self):\n",
    "        return self.space2\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        r = tf.subtract(1.0,inputs)\n",
    "        r = tf.multiply(r, self.space)\n",
    "        r = tf.cast(r, dtype = tf.float32)\n",
    "        return r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(size, stride, input_tensor):\n",
    "    x = keras.layers.Conv2D(size[0],(1, 1), padding = \"same\")(input_tensor)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(activations.relu)(x)\n",
    "    x = keras.layers.Conv2D(size[1],(stride, stride), padding = \"same\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(activations.relu)(x)\n",
    "    x = keras.layers.Conv2D(size[2],(1, 1), padding = \"same\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(activations.relu)(x)\n",
    "    \n",
    "    shotcut = keras.layers.Conv2D(size[2],(1, 1), padding = \"same\")(input_tensor)\n",
    "    shotcut = keras.layers.Activation(activations.relu)(shotcut)\n",
    "\n",
    "    x = keras.layers.Add()([x, shotcut])\n",
    "    x = keras.layers.Activation(activations.relu)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def identity_block(size, strike, input_tensor):\n",
    "    x = keras.layers.Conv2D(size[0],(1,1), padding = \"same\")(input_tensor)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(activations.relu)(x)\n",
    "    x = keras.layers.Conv2D(size[1],(3,3), padding = \"same\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(activations.relu)(x)\n",
    "    x = keras.layers.Conv2D(size[2],(1,1), padding = \"same\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(activations.relu)(x)\n",
    "\n",
    "    x = keras.layers.Add()([x, input_tensor])\n",
    "    x = keras.layers.Activation(activations.relu)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_tensor = keras.layers.Input(shape = [100,20,3], name = \"input\")\n",
    "\n",
    "#stage 1 \n",
    "x = noteFocus()(input_tensor)\n",
    "x = conv_block([64,64,64],1,input_tensor)\n",
    "x = identity_block([64,64,64],3,x)\n",
    "\n",
    "x = conv_block([64,64,64],3,input_tensor)\n",
    "x = identity_block([64,64,64],3,x)\n",
    "x = identity_block([64,64,64],3,x)\n",
    "x = identity_block([64,64,64],3,x)\n",
    "\n",
    "x = keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "#stage 2\n",
    "x = conv_block([128,128,128],3,x)\n",
    "x = identity_block([128,128,128],3,x)\n",
    "x = identity_block([128,128,128],3,x)\n",
    "x = identity_block([128,128,128],3,x)\n",
    "x = identity_block([128,128,128],3,x)\n",
    "\n",
    "x = keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "#stage 3\n",
    "x = conv_block([256,256,256],3,x)\n",
    "\n",
    "for i in range(4):\n",
    "    x = identity_block([256,256,256],3,x)\n",
    "x = keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "#stage 4\n",
    "x = conv_block([512,512,512],3,x)\n",
    "for i in range(5):\n",
    "    x = identity_block([512,512,512],3,x)\n",
    "\n",
    "x = keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "\n",
    "#stage 5\n",
    "a = keras.layers.Flatten()(x)\n",
    "a = keras.layers.Dense(8192, activation = 'relu')(a)\n",
    "a = keras.layers.Dense(4096, activation = 'relu')(a)\n",
    "a = keras.layers.Dense(2048, activation = 'relu')(a)\n",
    "a = keras.layers.Dense(1024, activation = 'relu')(a)\n",
    "a = keras.layers.Dense(512, activation = 'relu')(a)\n",
    "a = keras.layers.Dense(256, activation = 'relu')(a)\n",
    "a = keras.layers.Dense(128, activation = 'relu')(a)\n",
    "a = keras.layers.Dense(96, activation = 'relu')(a)\n",
    "a = keras.layers.Dense(48, activation = 'relu')(a)  \n",
    "\n",
    "\n",
    "output = keras.layers.Dense(len(c), activation = \"softmax\")(a)\n",
    "\n",
    "\n",
    "model  = keras.Model(inputs = [input_tensor], outputs = [output])\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMAGE AUGMENTATION \n",
    "\"\"\"\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "aug = ImageDataGenerator(zoom_range = [0.3,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRAINING \n",
    "\"\"\"\n",
    "import multiprocessing\n",
    "with tf.device('/CPU:0'):\n",
    "    his = model.fit_generator(aug.flow(trainM1_x, train_y), epochs = 5, steps_per_epoch =len(trainM2_x) , validation_steps = 300, validation_data=(valM1_x, val_y), shuffle=True, workers= multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TESTING\n",
    "\"\"\"\n",
    "prey = model.predict([testM1_x, testM2_x])\n",
    "maxv = []\n",
    "for ipred in range(len(prey)):\n",
    "    \n",
    "    maxs = 0\n",
    "    for i,index in enumerate(prey[ipred]):\n",
    "        if index > prey[ipred][maxs]:\n",
    "            maxs = i\n",
    "\n",
    "    maxv.append(maxs)\n",
    "\n",
    "print(maxv)\n",
    "c = ['G4','A4','B4','C3','D3','E3','F3','G3','A3','B3','C2','D2','E2','F2','G2','A2','B2','C','D','E','F','G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GIVEN TESTING DATA\n",
    "\"\"\"\n",
    "print(test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
